{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYQSK8ttexdIaqZhYy9RB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARJUN108-verma/NLP---Natural-Language-Processing/blob/main/4_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing Text:-"
      ],
      "metadata": {
        "id": "vrJMFN3a_w4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundamental step in NLP involves converting our text into smaller units through a process known as tokenization. These smaller units are known as our tokens. Word tokenization is the most common form of tokenization, where individual words in the text becomes a token, but tokens can also be sentences, sub words or individual characters depending on your use case.\n",
        "\n",
        "Why do we do this? The meaning of the overall text is better understood if we can analyse and understand the individual parts as well as the whole. It's also an important step before we vecotrize our data, which we'll cover more in the next section of this course.\n",
        "\n",
        "Now let's look at some examples of sentence and word tokenization using the nltk package."
      ],
      "metadata": {
        "id": "2gBhhEEN_2za"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "FjsFZkhi_pyl"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdwjCODSCsnD",
        "outputId": "a60ecccb-b4ce-4085-a0d0-14631c8f2d8a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "n-if5TXbC9mx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentance tokenization:-"
      ],
      "metadata": {
        "id": "RllyY8QiAclE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Her cat's name is Luna. Her dog's name is max\"\n",
        "sentences = sent_tokenize(sentence)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoI8WdekAIiP",
        "outputId": "1219a93e-50ef-4e9a-fd2f-3b0be94f4687"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Her cat's name is Luna.\", \"Her dog's name is max\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Tokenizer:-"
      ],
      "metadata": {
        "id": "vIOrGM31B9d8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"Her cat's name is Luna. Her dog's name is max\"\n",
        "words = word_tokenize(sentence)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0B49URrA8uJ",
        "outputId": "46c396ed-8353-46cb-dbbf-4bd7e999cb54"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Her', 'cat', \"'s\", 'name', 'is', 'Luna', '.', 'Her', 'dog', \"'s\", 'name', 'is', 'max']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how \"cat's\" has been split into 2 tokens. This may be fine for your task but it is definitely something to keep in mind when you are preprocessing any text data - you might want to remove punctuation or replace contractions before tokenizing."
      ],
      "metadata": {
        "id": "GJ7Ky7QhDpt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_2 = \"Her cat's name is Luna and her dog's name is max\"\n",
        "word_tokenize(sentence_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8qMBiPDCGwO",
        "outputId": "3bf1cad8-c7f7-4601-b40f-594b26b5b8b9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Her',\n",
              " 'cat',\n",
              " \"'s\",\n",
              " 'name',\n",
              " 'is',\n",
              " 'Luna',\n",
              " 'and',\n",
              " 'her',\n",
              " 'dog',\n",
              " \"'s\",\n",
              " 'name',\n",
              " 'is',\n",
              " 'max']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tokens illustrate what we learned in our last lesson about the importance of using lowercase. We can see we have two instances of the word 'her' - one which is capitalised. The tokens then are different and will be treated as different in most analysis."
      ],
      "metadata": {
        "id": "W-hy-7qFETPz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ss6Ub9gkDztp"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}